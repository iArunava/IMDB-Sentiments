{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "82a3f11267d24990d6ef9323c5c97b4f9af60f11"
   },
   "source": [
    "# IMDB Sentiments\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook follows the Text Classification guide from Google Machine Learning Guides.\n",
    "This notebook contains all the code that the guide shows in the tutorial and not in its github repo. Hope this guide helps you as you follow the Text Classification guide.\n",
    "\n",
    "Link to the Guide: https://developers.google.com/machine-learning/guides/text-classification/\n",
    "\n",
    "In this notebook, we see how to perform sentiment analysis using IMDB Movie Reviews Dataset. We will classify reviews into 2 labels: positive(1) and negetive(0). And we will encode the data using tf-idf and feed into a Multi-layer Perceptron. We will use tensorflow, with Keras API.\n",
    "\n",
    "Find the Kaggle Kernel here: https://www.kaggle.com/iarunava/google-text-classification-notebook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b9552fa0ddd512113e41bb41badf7bf86389a4e"
   },
   "source": [
    "## Loading the required modules\n",
    "\n",
    "Let;s get started by loading all the required modules and defining all the constants and variables that we will be needing all throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cd8836844c84ba8afe2b4724418b68fb1298b27",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path = './../input/aclimdb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da2eb32b4ececc890562532b3ef8ba0825414caf"
   },
   "source": [
    "## Load the Dataset\n",
    "\n",
    "In this section, let's load the dataset and shuffle it so to make ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8b776d6dc48490a553a768ae0f8f358f23ab20a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    perm = np.random.permutation(len(X))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b71385f428efe17e820f6f14da6e3f4cc4d78928",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb_dataset(path):\n",
    "    imdb_path = os.path.join(path, 'aclImdb')\n",
    "\n",
    "    # Load the dataset\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for dset in ['train', 'test']:\n",
    "        for cat in ['pos', 'neg']:\n",
    "            dset_path = os.path.join(imdb_path, dset, cat)\n",
    "            for fname in sorted(os.listdir(dset_path)):\n",
    "                if fname.endswith('.txt'):\n",
    "                    with open(os.path.join(dset_path, fname)) as f:\n",
    "                        if dset == 'train': train_texts.append(f.read())\n",
    "                        else: test_texts.append(f.read())\n",
    "                    label = 0 if cat == 'neg' else 1\n",
    "                    if dset == 'train': train_labels.append(label)\n",
    "                    else: test_labels.append(label)\n",
    "\n",
    "    # Converting to np.array\n",
    "    train_texts = np.array(train_texts)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_texts = np.array(test_texts)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    train_texts, train_labels = shuffle(train_texts, train_labels)\n",
    "    test_texts, test_labels = shuffle(test_texts, test_labels)\n",
    "\n",
    "    # Return the dataset\n",
    "    return train_texts, train_labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa744957e3aa9a8d007538e59cdfff7bc94f498e"
   },
   "source": [
    "Now, lets load the dataset and perform some analysis on the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1ff87c4c3848bfe182fefeb2e5a52e389e2f89f"
   },
   "outputs": [],
   "source": [
    "trX, trY, ttX, ttY = load_imdb_dataset(path)\n",
    "\n",
    "print ('Train samples shape :', trX.shape)\n",
    "print ('Train labels shape  :', trY.shape)\n",
    "print ('Test samples shape  :', ttX.shape)\n",
    "print ('Test labels shape   :', ttY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13bb99daf735823bc135a19f292f0cd0f7bea1b0"
   },
   "source": [
    "Okay, that's 25K samples in each train and test sets! Now, from here on we will do analysis only on the train set (we want no snooping bias!)\n",
    "\n",
    "Alright, now we have `2` classes as we divided them, one for positive `1` and one for negetive `0`. Let's just verify that!\n",
    "\n",
    "And let's also verify the number of samples that are present in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec618838c0f8741718adf2e184d2cc28bdb2c90b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq_class_arr, counts = np.unique(trY, return_counts=True)\n",
    "\n",
    "print ('Unique classes :', uniq_class_arr)\n",
    "print ('Number of unique classes : ', len(uniq_class_arr))\n",
    "\n",
    "for _class in uniq_class_arr:\n",
    "    print ('Counts for class ', uniq_class_arr[_class], ' : ', counts[_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7ae24d4326549e9025d5f5f653ff653772407d2"
   },
   "source": [
    "Okay, so that's expected! So, everything's fine!\n",
    "\n",
    "Now, let;s take a few random samples and check if the labels are expected!\n",
    "\n",
    "And, the counts for each class are also even!! Each class has `12500` samples! Alright!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb8352ec0f7ff218e4cd2495a0950e50f9dabf74",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_of_samp = 10\n",
    "rand_samples_to_check = np.random.randint(len(trX), size=size_of_samp)\n",
    "\n",
    "for samp_num in rand_samples_to_check:\n",
    "    print ('============================================================')\n",
    "    print (trX[samp_num], '||', trY[samp_num])\n",
    "    print ('============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d61b617dfa8dafc1fffc9a63f2ecb6eaefe6721"
   },
   "source": [
    "Okay, so reading the reviews, the labels are expected, so we are good!\n",
    "\n",
    "Now, let's see the average number of words per sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71bb6888ea549ffad42930e7965655bf00676039"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist([len(sample) for sample in list(trX)], 50)\n",
    "plt.xlabel('Length of samples')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Sample length distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c03be6fcf44c0e6a8a607b34a0d63d8d8d0f8d00"
   },
   "source": [
    "Let's now plot a frequency distribution plot of the most seen words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dda5078d462678577cdf3986b9feda410c4cbed1"
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'ngram_range' : (1, 1),\n",
    "    'dtype' : 'int32',\n",
    "    'strip_accents' : 'unicode',\n",
    "    'decode_error' : 'replace',\n",
    "    'analyzer' : 'word'\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(**kwargs)\n",
    "vect_texts = vectorizer.fit_transform(list(trX))\n",
    "all_ngrams = vectorizer.get_feature_names()\n",
    "num_ngrams = min(50, len(all_ngrams))\n",
    "all_counts = vect_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "all_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\n",
    "ngrams = all_ngrams[:num_ngrams]\n",
    "counts = all_counts[:num_ngrams]\n",
    "\n",
    "idx = np.arange(num_ngrams)\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.bar(idx, counts, width=0.8)\n",
    "plt.xlabel('N-grams')\n",
    "plt.ylabel('Frequencies')\n",
    "plt.title('Frequency distribution of ngrams')\n",
    "plt.xticks(idx, ngrams, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68582f67284c30edf39adc574483f55d3733fdb1",
    "collapsed": true
   },
   "source": [
    "Well, the highest frequency words are the stop words. We not consider them while performing our analysis, as they don't provide insights as to what the sentiment of the document might be or to which class a document might belong."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
