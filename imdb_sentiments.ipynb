{"cells":[{"metadata":{"_uuid":"82a3f11267d24990d6ef9323c5c97b4f9af60f11"},"cell_type":"markdown","source":"# IMDB Sentiments\n\n## Introduction\n\nIn this notebook, we see how to perform sentiment analysis using IMDB Movie Reviews Dataset. We will classify reviews into `2` labels: _positive(`1`)_ and _negetive(`0`)_"},{"metadata":{"_uuid":"2b9552fa0ddd512113e41bb41badf7bf86389a4e"},"cell_type":"markdown","source":"## Loading the required modules\n\nLet;s get started by loading all the required modules and defining all the constants and variables that we will be needing all throughout the notebook"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3cd8836844c84ba8afe2b4724418b68fb1298b27"},"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\npath = './../input/aclimdb/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da2eb32b4ececc890562532b3ef8ba0825414caf"},"cell_type":"markdown","source":"## Load the Dataset\n\nIn this section, let's load the dataset and shuffle it so to make ready for analysis."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d8b776d6dc48490a553a768ae0f8f358f23ab20a"},"cell_type":"code","source":"def shuffle(X, y):\n    perm = np.random.permutation(len(X))\n    X = X[perm]\n    y = y[perm]\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b71385f428efe17e820f6f14da6e3f4cc4d78928"},"cell_type":"code","source":"def load_imdb_dataset(path):\n    imdb_path = os.path.join(path, 'aclImdb')\n\n    # Load the dataset\n    train_texts = []\n    train_labels = []\n    test_texts = []\n    test_labels = []\n    for dset in ['train', 'test']:\n        for cat in ['pos', 'neg']:\n            dset_path = os.path.join(imdb_path, dset, cat)\n            for fname in sorted(os.listdir(dset_path)):\n                if fname.endswith('.txt'):\n                    with open(os.path.join(dset_path, fname)) as f:\n                        if dset == 'train': train_texts.append(f.read())\n                        else: test_texts.append(f.read())\n                    label = 0 if cat == 'neg' else 1\n                    if dset == 'train': train_labels.append(label)\n                    else: test_labels.append(label)\n\n    # Converting to np.array\n    train_texts = np.array(train_texts)\n    train_labels = np.array(train_labels)\n    test_texts = np.array(test_texts)\n    test_labels = np.array(test_labels)\n\n    # Shuffle the dataset\n    train_texts, train_labels = shuffle(train_texts, train_labels)\n    test_texts, test_labels = shuffle(test_texts, test_labels)\n\n    # Return the dataset\n    return train_texts, train_labels, test_texts, test_labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa744957e3aa9a8d007538e59cdfff7bc94f498e"},"cell_type":"markdown","source":"Now, lets load the dataset and perform some analysis on the dataset!"},{"metadata":{"trusted":true,"_uuid":"b1ff87c4c3848bfe182fefeb2e5a52e389e2f89f"},"cell_type":"code","source":"trX, trY, ttX, ttY = load_imdb_dataset(path)\n\nprint ('Train samples shape :', trX.shape)\nprint ('Train labels shape  :', trY.shape)\nprint ('Test samples shape  :', ttX.shape)\nprint ('Test labels shape   :', ttY.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13bb99daf735823bc135a19f292f0cd0f7bea1b0"},"cell_type":"markdown","source":"Okay, that's 25K samples in each train and test sets! Now, from here on we will do analysis only on the train set (we want no snooping bias!)\n\nAlright, now we have `2` classes as we divided them, one for positive `1` and one for negetive `0`. Let's just verify that!\n\nAnd let's also verify the number of samples that are present in each class."},{"metadata":{"trusted":true,"_uuid":"ec618838c0f8741718adf2e184d2cc28bdb2c90b","collapsed":true},"cell_type":"code","source":"uniq_class_arr, counts = np.unique(trY, return_counts=True)\n\nprint ('Unique classes :', uniq_class_arr)\nprint ('Number of unique classes : ', len(uniq_class_arr))\n\nfor _class in uniq_class_arr:\n    print ('Counts for class ', uniq_class_arr[_class], ' : ', counts[_class])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ae24d4326549e9025d5f5f653ff653772407d2"},"cell_type":"markdown","source":"Okay, so that's expected! So, everything's fine!\n\nNow, let;s take a few random samples and check if the labels are expected!\n\nAnd, the counts for each class are also even!! Each class has `12500` samples! Alright!"},{"metadata":{"trusted":true,"_uuid":"eb8352ec0f7ff218e4cd2495a0950e50f9dabf74","collapsed":true},"cell_type":"code","source":"size_of_samp = 10\nrand_samples_to_check = np.random.randint(len(trX), size=size_of_samp)\n\nfor samp_num in rand_samples_to_check:\n    print ('============================================================')\n    print (trX[samp_num], '||', trY[samp_num])\n    print ('============================================================')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d61b617dfa8dafc1fffc9a63f2ecb6eaefe6721"},"cell_type":"markdown","source":"Okay, so reading the reviews, the labels are expected, so we are good!\n\nNow, let's see the average number of words per sample!"},{"metadata":{"trusted":true,"_uuid":"71bb6888ea549ffad42930e7965655bf00676039"},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.hist([len(sample) for sample in list(trX)], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c03be6fcf44c0e6a8a607b34a0d63d8d8d0f8d00"},"cell_type":"markdown","source":"Let's now plot a frequency distribution plot of the most seen words in the corpus."},{"metadata":{"trusted":true,"_uuid":"dda5078d462678577cdf3986b9feda410c4cbed1"},"cell_type":"code","source":"kwargs = {\n    'ngram_range' : (1, 1),\n    'dtype' : 'int32',\n    'strip_accents' : 'unicode',\n    'decode_error' : 'replace',\n    'analyzer' : 'word'\n}\n\nvectorizer = CountVectorizer(**kwargs)\nvect_texts = vectorizer.fit_transform(list(trX))\nall_ngrams = vectorizer.get_feature_names()\nnum_ngrams = min(50, len(all_ngrams))\nall_counts = vect_texts.sum(axis=0).tolist()[0]\n\nall_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\nngrams = all_ngrams[:num_ngrams]\ncounts = all_counts[:num_ngrams]\n\nidx = np.arange(num_ngrams)\n\nplt.figure(figsize=(30, 30))\nplt.bar(idx, counts, width=0.8)\nplt.xlabel('N-grams')\nplt.ylabel('Frequencies')\nplt.title('Frequency distribution of ngrams')\nplt.xticks(idx, ngrams, rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"68582f67284c30edf39adc574483f55d3733fdb1"},"cell_type":"markdown","source":"Well, the highest frequency words are the stop words. We not consider them while performing our analysis, as they don't provide insights as to what the sentiment of the document might be or to which class a document might belong."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}