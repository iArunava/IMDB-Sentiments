{"cells":[{"metadata":{"_uuid":"82a3f11267d24990d6ef9323c5c97b4f9af60f11"},"cell_type":"markdown","source":"# IMDB Sentiments\n\n## Introduction\n\nThis notebook follows the Text Classification guide from Google Machine Learning Guides.<br/>\nThis notebook contains all the code that the guide shows in the tutorial and not in its github repo. Hope this guide helps you as you follow the Text Classification guide.\n\nLink to the Guide: https://developers.google.com/machine-learning/guides/text-classification/\n\nIn this notebook, we see how to perform sentiment analysis using IMDB Movie Reviews Dataset. We will classify reviews into `2` labels: _positive(`1`)_ and _negetive(`0`)_. And we will encode the data using tf-idf and feed into a Multi-layer Perceptron. We will use tensorflow, with Keras API."},{"metadata":{"_uuid":"2b9552fa0ddd512113e41bb41badf7bf86389a4e"},"cell_type":"markdown","source":"## Loading the required modules\n\nLet;s get started by loading all the required modules and defining all the constants and variables that we will be needing all throughout the notebook"},{"metadata":{"trusted":true,"_uuid":"3cd8836844c84ba8afe2b4724418b68fb1298b27","collapsed":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout\n\npath = './../input/aclimdb/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da2eb32b4ececc890562532b3ef8ba0825414caf"},"cell_type":"markdown","source":"## Load the Dataset\n\nIn this section, let's load the dataset and shuffle it so to make ready for analysis."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d8b776d6dc48490a553a768ae0f8f358f23ab20a"},"cell_type":"code","source":"def shuffle(X, y):\n    perm = np.random.permutation(len(X))\n    X = X[perm]\n    y = y[perm]\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b71385f428efe17e820f6f14da6e3f4cc4d78928"},"cell_type":"code","source":"def load_imdb_dataset(path):\n    imdb_path = os.path.join(path, 'aclImdb')\n\n    # Load the dataset\n    train_texts = []\n    train_labels = []\n    test_texts = []\n    test_labels = []\n    for dset in ['train', 'test']:\n        for cat in ['pos', 'neg']:\n            dset_path = os.path.join(imdb_path, dset, cat)\n            for fname in sorted(os.listdir(dset_path)):\n                if fname.endswith('.txt'):\n                    with open(os.path.join(dset_path, fname)) as f:\n                        if dset == 'train': train_texts.append(f.read())\n                        else: test_texts.append(f.read())\n                    label = 0 if cat == 'neg' else 1\n                    if dset == 'train': train_labels.append(label)\n                    else: test_labels.append(label)\n\n    # Converting to np.array\n    train_texts = np.array(train_texts)\n    train_labels = np.array(train_labels)\n    test_texts = np.array(test_texts)\n    test_labels = np.array(test_labels)\n\n    # Shuffle the dataset\n    train_texts, train_labels = shuffle(train_texts, train_labels)\n    test_texts, test_labels = shuffle(test_texts, test_labels)\n\n    # Return the dataset\n    return train_texts, train_labels, test_texts, test_labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa744957e3aa9a8d007538e59cdfff7bc94f498e"},"cell_type":"markdown","source":"Now, lets load the dataset and perform some analysis on the dataset!"},{"metadata":{"trusted":true,"_uuid":"b1ff87c4c3848bfe182fefeb2e5a52e389e2f89f","collapsed":true},"cell_type":"code","source":"trX, trY, ttX, ttY = load_imdb_dataset(path)\n\nprint ('Train samples shape :', trX.shape)\nprint ('Train labels shape  :', trY.shape)\nprint ('Test samples shape  :', ttX.shape)\nprint ('Test labels shape   :', ttY.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13bb99daf735823bc135a19f292f0cd0f7bea1b0"},"cell_type":"markdown","source":"Okay, that's 25K samples in each train and test sets! Now, from here on we will do analysis only on the train set (we want no snooping bias!)\n\nAlright, now we have `2` classes as we divided them, one for positive `1` and one for negetive `0`. Let's just verify that!\n\nAnd let's also verify the number of samples that are present in each class."},{"metadata":{"trusted":true,"_uuid":"ec618838c0f8741718adf2e184d2cc28bdb2c90b","collapsed":true},"cell_type":"code","source":"uniq_class_arr, counts = np.unique(trY, return_counts=True)\n\nprint ('Unique classes :', uniq_class_arr)\nprint ('Number of unique classes : ', len(uniq_class_arr))\n\nfor _class in uniq_class_arr:\n    print ('Counts for class ', uniq_class_arr[_class], ' : ', counts[_class])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ae24d4326549e9025d5f5f653ff653772407d2"},"cell_type":"markdown","source":"Okay, so that's expected! So, everything's fine!\n\nNow, let;s take a few random samples and check if the labels are expected!\n\nAnd, the counts for each class are also even!! Each class has `12500` samples! Alright!"},{"metadata":{"trusted":true,"_uuid":"eb8352ec0f7ff218e4cd2495a0950e50f9dabf74","collapsed":true},"cell_type":"code","source":"size_of_samp = 10\nrand_samples_to_check = np.random.randint(len(trX), size=size_of_samp)\n\nfor samp_num in rand_samples_to_check:\n    print ('============================================================')\n    print (trX[samp_num], '||', trY[samp_num])\n    print ('============================================================')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d61b617dfa8dafc1fffc9a63f2ecb6eaefe6721"},"cell_type":"markdown","source":"Okay, so reading the reviews, the labels are expected, so we are good!\n\nNow, let's see the average number of words per sample!"},{"metadata":{"trusted":true,"_uuid":"71bb6888ea549ffad42930e7965655bf00676039","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.hist([len(sample) for sample in list(trX)], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c03be6fcf44c0e6a8a607b34a0d63d8d8d0f8d00"},"cell_type":"markdown","source":"Let's now plot a frequency distribution plot of the most seen words in the corpus."},{"metadata":{"trusted":true,"_uuid":"dda5078d462678577cdf3986b9feda410c4cbed1","collapsed":true},"cell_type":"code","source":"kwargs = {\n    'ngram_range' : (1, 1),\n    'dtype' : 'int32',\n    'strip_accents' : 'unicode',\n    'decode_error' : 'replace',\n    'analyzer' : 'word'\n}\n\nvectorizer = CountVectorizer(**kwargs)\nvect_texts = vectorizer.fit_transform(list(trX))\nall_ngrams = vectorizer.get_feature_names()\nnum_ngrams = min(50, len(all_ngrams))\nall_counts = vect_texts.sum(axis=0).tolist()[0]\n\nall_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\nngrams = all_ngrams[:num_ngrams]\ncounts = all_counts[:num_ngrams]\n\nidx = np.arange(num_ngrams)\n\nplt.figure(figsize=(30, 30))\nplt.bar(idx, counts, width=0.8)\nplt.xlabel('N-grams')\nplt.ylabel('Frequencies')\nplt.title('Frequency distribution of ngrams')\nplt.xticks(idx, ngrams, rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"68582f67284c30edf39adc574483f55d3733fdb1"},"cell_type":"markdown","source":"Well, the highest frequency words are the stop words. We not consider them while performing our analysis, as they don't provide insights as to what the sentiment of the document might be or to which class a document might belong."},{"metadata":{"_uuid":"46d3205e9a9a6eb29d69a49aa8514ad87da8b25b"},"cell_type":"markdown","source":"## Prepare the data\n\nLet's now prepare the data to feed into the model. For the data preparation step we will get bigrams and unigrams from the data and encode it using tf-idf. And will select the top `20000` features from the vector of tokens. Discard features that occurs less than two times, and will `f_classif` to get feature importance."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6e348dae98b6105963dbbcbf85553ddbafa1e95e"},"cell_type":"code","source":"NGRAM_RANGE = (1, 2)\nTOP_K = 20000\nTOKEN_MODE = 'word'\nMIN_DOC_FREQ = 2\n\ndef ngram_vectorize(train_texts, train_labels, val_texts):\n    kwargs = {\n        'ngram_range' : NGRAM_RANGE,\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : TOKEN_MODE,\n        'min_df' : MIN_DOC_FREQ,\n    }\n    \n    # Learn Vocab from train texts and vectorize train and val sets\n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    x_train = tfidf_vectorizer.fit_transform(train_texts)\n    x_val = tfidf_vectorizer.transform(val_texts)\n    \n    # Select best k features, with feature importance measured by f_classif\n    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n    selector.fit(x_train, train_labels)\n    x_train = selector.transform(x_train).astype('float32')\n    x_val = selector.transform(x_val).astype('float32')\n    return x_train, x_val","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b793569b4cea6af13360ab71811f5cb07b0431b"},"cell_type":"markdown","source":"## Build, Train and Evaluate the model\n\nFirst, let's create a function that returns the appropriate number of units and the activation for the last layer."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7345831d90c6cde6422006973ae0afc1f48ea948"},"cell_type":"code","source":"def get_last_layer_units_and_activation(num_classes):\n    if num_classes == 2:\n        activation = 'sigmoid'\n        units = 1\n    else:\n        activation = 'softmax'\n        units = num_classes\n    return units, activation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c17b9237367ada6c83101a8fee65549e54b4474"},"cell_type":"markdown","source":"Let's now create the model using the Keras API from tensorflow"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cfa27ca95464d2413ec0ba9954a436d57c4ffc5"},"cell_type":"code","source":"def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n    op_units, op_activation = get_last_layer_units_and_activation(num_classes)\n    model = models.Sequential()\n    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n    \n    for _ in range(layers-1):\n        model.add(Dense(units=units, activation='relu'))\n        model.add(Dropout(rate=dropout_rate))\n        \n    model.add(Dense(units=op_units, activation=op_activation))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1afdef7b79aa7a1b5bb7c36926d3481f37da834"},"cell_type":"markdown","source":"Now, let's train the model"},{"metadata":{"trusted":true,"_uuid":"4340b578aa13c51aa3469c78b9d75e50373fc688","collapsed":true},"cell_type":"code","source":"def train_ngram_model(data, learning_rate=1e-3, epochs=1000, batch_size=128, layers=2, units=64, \n                      dropout_rate=0.2):\n    \n    num_classes = 2\n    \n    # Get the data\n    trX, trY, ttX, ttY = data\n    \n    # Verify the validation labels\n    '''\n    unexpected_labels = [v for v in ttY if v not in range(num_classes)]\n    if len(unexpected_labels):\n        raise ValueError('Unexpected label values found in the validation set:'\n                         ' {unexpected_labels}. Please make sure that the labels'\n                         ' in the validation set are in the same range as '\n                         'training labels.'.format(unexpected_labels=unexpected_labels))\n    '''\n    \n    # Vectorize the data\n    x_train, x_val = ngram_vectorize(trX, trY, ttX)\n    \n    # Create model instance\n    model = mlp_model(layers, units=units, dropout_rate=dropout_rate,\n                      input_shape=x_train.shape[1:], num_classes=num_classes)\n    \n    # Compile model with parameters\n    if num_classes == 2:\n        loss = 'binary_crossentropy'\n    else:\n        loss = 'sparse_categorical_crossentropy'\n    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n    \n    # Create callback for early stopping on validation loss. If the loss does\n    # not decrease on two consecutive tries, stop training\n    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n    \n    # Train and validate model\n    history = model.fit(x_train, trY, epochs=epochs, validation_data=(x_val, ttY),\n                        verbose=2, batch_size=batch_size, callbacks=callbacks)\n    \n    # Print results\n    history = history.history\n    val_acc = history['val_acc'][-1]\n    val_loss = history['val_loss'][-1]\n    print ('Validation accuracy: {acc}, loss: {loss}'.format(\n            acc=val_acc, loss=val_loss))\n    \n    # Save model\n    model.save('IMDB_mlp_model_' + str(val_acc) + '_' + str(loss) + '.h5')\n    return val_acc, val_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1574c8831ef8f4d6724e2dec84c24818cf6cdd8"},"cell_type":"markdown","source":"Alright!!!!<br/>\nLet's now call `train_ngram_model` and build the model!!"},{"metadata":{"trusted":true,"_uuid":"f40e5cc25d4456cfc05de5a729e3c94d5918f7f4","scrolled":true,"collapsed":true},"cell_type":"code","source":"results = train_ngram_model((trX, trY, ttX, ttY))\n\nprint ('With lr=1e-3 | val_acc={results[0]} | val_loss={results[1]}'.format(results=results))\nprint ('===========================================================================================')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"812caafc35a867cac96adf99d61ccc6f7ad28bce","collapsed":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3cd0d62d5feafed32e01f9c158df8e23dcf4a0e"},"cell_type":"markdown","source":"Above we can see we are getting a whooping `90`% accuracy!!!<br/>\nBut take a look at the `acc`, its getting a `97`% accuracy! This indicates a clear overfitting problem."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93fc37e52a0e870f8abd3d3351b6a64fc1c80e5d"},"cell_type":"markdown","source":"## Tune Hyperparameters\n\nThe above model is not tuned. So, take your time to find the best set of hyperparameters!<br/>\nSee this page for more details: https://developers.google.com/machine-learning/guides/text-classification/step-5"},{"metadata":{"_uuid":"9e8ed96a46772fc368e92184b04d2b0ef8a268c7"},"cell_type":"markdown","source":"## Deploy your model\n\nNow, go deploy your model. See here: https://developers.google.com/machine-learning/guides/text-classification/step-6"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}